{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5KRLS82Y_PX"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKnBD3tLY844",
        "outputId": "5959a8d3-d449-4655-f2f0-0dd81ed20b44"
      },
      "outputs": [],
      "source": [
        "# import module for tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "# input the text\n",
        "text = \"\"\"\n",
        "An HTTP request is made by a client, to a named host, which is located on a server. \n",
        "The aim of the request is to access a resource on the server. To make the request, the client uses components of a URL (Uniform Resource Locator), \n",
        "which includes the information needed to access the resource.\n",
        "\"\"\"\n",
        "word_tokenize(text)\n",
        "# Challenge: on first try, you may not be run into errors \"resource_not_found\". Attempt to solve this error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ivc3KeLa6HN"
      },
      "outputs": [],
      "source": [
        "# Mission: assign the tokens to an array named 'tokens'\n",
        "tokens = word_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3NYJs0Bb9-J"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u3KytuqcIz0",
        "outputId": "980ad3bf-98f9-45ea-df62-9b368327b558"
      },
      "outputs": [],
      "source": [
        "#import the module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# initiate the lemmatizer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "# Challenge: on first try, you may not be run into errors \"resource_not_found\". Attempt to solve this error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGmVAaP8cZlz",
        "outputId": "3701af05-ce53-4188-c6cc-e0070337a5b6"
      },
      "outputs": [],
      "source": [
        "# Mission: Lemmatize all the words in the tokens array\n",
        "new_tokens = []\n",
        "for token in tokens:\n",
        "  new_token = lemmatizer.lemmatize(token)\n",
        "  print(token + ' :' + new_token )\n",
        "  new_tokens.append(new_token)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xka1cCgKeYfb"
      },
      "source": [
        "Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rS44PQAQcv0Z",
        "outputId": "0d53ddda-97bd-4c86-e8d7-79632796821f"
      },
      "outputs": [],
      "source": [
        "# import the module\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_tokens = [token for token in new_tokens if not token.lower() in stop_words]\n",
        "\n",
        "print(filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('BuildingBloCS', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('yearly', 'JJ'), ('event', 'NN'), ('that', 'WDT'), ('teaches', 'VBZ'), ('student', 'NN'), ('computing', 'VBG'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "text = \"BuildingBloCS is a yearly event that teaches student computing.\"\n",
        "sentence = nltk.sent_tokenize(text)\n",
        "for sent in sentence:\n",
        "\t print(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
        "# now add your own text to tag it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "USING RAKE ALGORITHM TO EXTRACT KEYWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TO INSTALL RAKE MODULE\n",
        "!pip install rake-nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rake_nltk import Rake\n",
        "r = Rake()\n",
        "text = \"\"\"\n",
        "An HTTP request is made by a client, to a named host, which is located on a server. \n",
        "The aim of the request is to access a resource on the server. To make the request, the client uses components of a URL (Uniform Resource Locator), \n",
        "which includes the information needed to access the resource.\n",
        "\"\"\"\n",
        "r.extract_keywords_from_text(text)\n",
        "r.get_ranked_phrases()\n",
        "\n",
        "# Mission: One of the results this algorithm outputs is 'uniform resource locator )'. \n",
        "# Clean the text by removing punctuation marks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spacy to extract keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#install spacy module\n",
        "!pip3 install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def get_hotwords(text):\n",
        "    result = []\n",
        "    pos_tag = ['PROPN', 'ADJ', 'NOUN'] \n",
        "    doc = nlp(text.lower()) \n",
        "    for token in doc:\n",
        "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
        "            continue\n",
        "        if(token.pos_ in pos_tag):\n",
        "            result.append(token.text)\n",
        "    return result\n",
        "text = \"\"\"\n",
        "An HTTP request is made by a client, to a named host, which is located on a server. \n",
        "The aim of the request is to access a resource on the server. To make the request, the client uses components of a URL (Uniform Resource Locator), \n",
        "which includes the information needed to access the resource.\n",
        "\"\"\"\n",
        "output = set(get_hotwords(text))\n",
        "most_common_list = Counter(output).most_common(10)\n",
        "for item in most_common_list:\n",
        "  print(item[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pytextrank to extract keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install pytextrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pytextrank\n",
        "# example text\n",
        "text = \"\"\"\n",
        "An HTTP request is made by a client, to a named host, which is located on a server. \n",
        "The aim of the request is to access a resource on the server. To make the request, the client uses components of a URL (Uniform Resource Locator), \n",
        "which includes the information needed to access the resource.\n",
        "\"\"\"\n",
        "# load a spaCy model, depending on language, scale, etc.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# add PyTextRank to the spaCy pipeline\n",
        "nlp.add_pipe(\"textrank\")\n",
        "doc = nlp(text)\n",
        "# examine the top-ranked phrases in the document\n",
        "for phrase in doc._.phrases[:10]:\n",
        "    print(phrase.text)\n",
        "\n",
        "# there are multiple results with 'a' as part of the keyphrase,\n",
        "# can we possible remove them for they keywords to be better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementation for the TRA. \n",
        "Using a technique you have learnt just now OR using a new method you have found\n",
        "Design a function that takes in a bunch of sentences, and return a list with \n",
        "1. the sentence with the keywords replaced with blanks '_'\n",
        "2. the keywords\n",
        "eg. The mitochondria is the powerhouse of the cell\n",
        "returns\n",
        "['The _______ is the _______ of the cell', ['mitochondria','powerhouse']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP tools",
      "provenance": []
    },
    "interpreter": {
      "hash": "cdac3f338c4a9266f15b7211e0fd6aeed3fb6bd8b14dd7217ff2a9498d21b47d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
